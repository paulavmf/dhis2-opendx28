{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6181e32a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.461218110Z",
     "start_time": "2023-07-19T11:53:07.409463379Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import string\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate unic id\n",
    "def generate_id(long:int):\n",
    "    # def characters\n",
    "    characters = string.ascii_letters + string.digits\n",
    "\n",
    "    # Generate random id\n",
    "    generated_id = ''.join(random.choice(characters) for _ in range(long))\n",
    "\n",
    "    return generated_id\n",
    "# Save a detailed log\n",
    "def changeLog(txt:str):\n",
    "    current_time = datetime.datetime.now()\n",
    "    log_txt = f\"{current_time}: \"+f\"{txt}\"+f'\\n'\n",
    "\n",
    "    # folder name\n",
    "    log_folder = \"./logs\"\n",
    "    # logfile name\n",
    "    logFile = os.path.join(log_folder, f\"log{datetime.date.today()}.txt\")\n",
    "    try:\n",
    "        with open(logFile,\"a\") as file:\n",
    "            file.write(log_txt)\n",
    "    except:\n",
    "        # create folder if not exist\n",
    "        if not os.path.exists(log_folder):\n",
    "            os.makedirs(logFile)\n",
    "        # create file if not exist\n",
    "        if not os.path.exists(logFile):\n",
    "            open(logFile, 'w').close()\n",
    "\n",
    "# Get Info about DataSets filtered by name\n",
    "def getDataSetInfoByName(name):\n",
    "    \"\"\"\n",
    "    :param name: name used to filter in to data sets\n",
    "    :return: This method return the basic information about specific dataset filtered by id\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session = login()\n",
    "        url = f\"http://localhost:8080/api/dataSets.json?\"\n",
    "        params = {\n",
    "            \"filter\":f\"name:like:{name}\"\n",
    "        }\n",
    "        response = session.get(url, headers={'content-type': 'application/json'}, params=params)\n",
    "        changeLog(f\"[FETCHING DATA-SET INFO] >> {response.text}\")\n",
    "        return response.text\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "# Get Info about DataSets filtered by name\n",
    "def getOrgUnitInfoByName(name):\n",
    "    \"\"\"\n",
    "    :param name: name used to filter in to OrgUnits\n",
    "    :return: This method return the basic information about specific organisation unit filtered by id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = login()\n",
    "        url = f\"http://localhost:8080/api/organisationUnits.json?\"\n",
    "        params = {\n",
    "            \"filter\": f\"name:like:{name}\"\n",
    "        }\n",
    "        response = session.get(url, headers={'content-type': 'application/json'}, params=params)\n",
    "        changeLog(f\"[FETCHING ORG-UNIT INFO] >> {response.text}\")\n",
    "        return response.text\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "# Get Info about DataElements filtered by name\n",
    "def getDataElementInfoByName(name):\n",
    "    \"\"\"\n",
    "    :param name: name used to filter inn to Data Elements\n",
    "    :return:  This method return the basic information about specific data element filtered by id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = login()\n",
    "        url = f\"http://localhost:8080/api/dataElements.json?\"\n",
    "        params = {\n",
    "            \"filter\": f\"name:like:{name}\"\n",
    "        }\n",
    "        response = session.get(url, headers={'content-type': 'application/json'}, params=params)\n",
    "        changeLog(f\"[FETCHING DATA-ELEMENT INFO] >> {response.text}\")\n",
    "        return response.text\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "# Generate a session to makes request\n",
    "def login():\n",
    "    session = requests.Session()\n",
    "    session.auth = ('admin', 'district')\n",
    "    return session\n",
    "\n",
    "def createDataElement(code:str,name:str, shortName:str,agregationType:str,domainType:str,valueType:str,zeroIsSignificant:bool):  # create a new data element into dhis\n",
    "    \"\"\"\n",
    "    :param code:\n",
    "    :param name:\n",
    "    :param shortName:\n",
    "    :param agregationType: [SUM,AVERAGE,MIN,MAX,COUNT,FIRST,LAST,STDDEV,VARIANCE,PERCENTILE,MEDIAN]\n",
    "    :param domainType: [AGGREGATE,TRACKER]\n",
    "    :param valueType: [TEXT,NUMBER,DATE,BOOLEAN,OPTION (like array list),ORGANISATION_UNIT,FILE_RESOURCE,COORDINATE,URL,CALCULATED_VALUE (for formula)\n",
    "    :param zeroIsSignificant: True/False\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    NOTA el category combo es usado para agrupar datos y asociarlos a un id por ejemplo edades o elementos similes\n",
    "    no es buena idea crear un id combo cada vez que se envian datos, hay que establecer un procedimiento para segun los\n",
    "    datos que se quieran agrupar se genere un combo nuevo para esos datos\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "    \"code\": code,\n",
    "    \"name\": name,\n",
    "    \"shortName\": shortName,\n",
    "    \"aggregationType\": agregationType,\n",
    "    \"domainType\": domainType,\n",
    "    \"valueType\": valueType,\n",
    "    \"zeroIsSignificant\": zeroIsSignificant,\n",
    "    \"categoryCombo\": {\n",
    "        \"id\": \"bjDvmb4bfuf\"\n",
    "        }\n",
    "    }\n",
    "    # changeLog(\"Generated payload with id: \"+catComboID)\n",
    "    # sending data\n",
    "    try:\n",
    "        session = login()\n",
    "        url = \"http://localhost:8080/api/dataElements\"\n",
    "        response = session.post(url, json=payload, headers={'content-type': 'application/json'})\n",
    "        changeLog(\"[CREATE DATA ELEMENT] >> \"+response.text)\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "def createDataSet(name:str,shortname:str,periodType:str,expiryDays:int,dataElementArr:[],orgUnitArr:[]):\n",
    "    \"\"\"\n",
    "    :param name:\n",
    "    :param shortname:\n",
    "    :param periodType: Monthly,\n",
    "    :param expiryDays: Limits of days until expire the data set ex: 365 or 1000 need to be int data type\n",
    "    :param dataElementArr: an array list with all ids related to data elements example arrDE = list([\"bDH5ZuaeyHH\"])\n",
    "    :param orgUnitArr: an arrat kust with the oute of organizations units starting from the root one\n",
    "            Example: 'Sierra Leone', 'otherOrgUnit inside'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE\n",
    "    - The data elements is refered by uid from db and can be seen using '/api/dataElements.json'\n",
    "    - This method automatically attach the data set to main org unit\n",
    "    \"\"\"\n",
    "    changeLog(f\"[CREATE DATA SET] >>  {len(dataElementArr)} DataElement(s) added to DataSet {name}\")\n",
    "    # Generate a data elements list to be added in payload dataSetElements section\n",
    "    DataElementsList = []\n",
    "    for x in dataElementArr:\n",
    "        dataElement = {\"dataElement\": {\"id\": x}}\n",
    "        changeLog(f\"[CREATE DATA SET] >>  {x} DataElement added to DataSet {name}\")\n",
    "        DataElementsList.append(dataElement)\n",
    "    # Generated a OrgUnit element list to be added in payload organisation units section\n",
    "    OrgUnitList = []\n",
    "    for x in orgUnitArr:\n",
    "        OrgUnit = {'id': x}\n",
    "        changeLog(f\"[CREATE DATA SET] >>  {x} OrgUnit added to DataSet {name}\")\n",
    "        OrgUnitList.append(OrgUnit)\n",
    "    payload = {\n",
    "        'name': name,  # Name of the dataset\n",
    "        'shortName': shortname,  # Short name for the dataset\n",
    "        'periodType': periodType,  # Period type for the dataset\n",
    "        'expiryDays': expiryDays,  # Number of days until dataset expires\n",
    "        \"dataSetElements\": DataElementsList,\n",
    "        \"sharing\": {\n",
    "            \"public\": \"rw------\",\n",
    "            \"external\": True,\n",
    "        },\n",
    "        \"organisationUnits\": OrgUnitList,\n",
    "        #ImspTQPwCqd\n",
    "    }\n",
    "    try:\n",
    "        session = login()\n",
    "        url = \"http://localhost:8080/api/dataSets\"\n",
    "        response = session.post(url, json=payload, headers={'content-type': 'application/json'})\n",
    "        changeLog(\"[CREATE DATA SET] \"+response.text)\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "\n",
    "# Used to create an organisation unit\n",
    "def createOrgUnit(code:str,name:str,shortname:str,preriodType:str):\n",
    "    \"\"\"\n",
    "    :param code:\n",
    "    :param name:\n",
    "    :param shortname:\n",
    "    :param preriodType:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"parent its Sierra leone instance or main instance of organization unit\"\"\"\n",
    "    payload = {\n",
    "        'code': code,\n",
    "        'name': name,\n",
    "        'shortName': shortname,\n",
    "        'openingDate': f\"{datetime.date.today().strftime('%Y-%m-%d')}\",\n",
    "        'periodType': preriodType,\n",
    "        \"parent\": {\n",
    "            \"id\": \"ImspTQPwCqd\",\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        session = login()\n",
    "        url = \"http://localhost:8080/api/organisationUnits\"\n",
    "        response = session.post(url, json=payload, headers={'content-type': 'application/json'})\n",
    "        changeLog(\"[CREATE ORGANISATION UNIT] >> \"+response.text)\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "# Used to upload dava values to specific dara set\n",
    "def addDataValue(DataValueSets,DataElement:str,DataSetName:str,OrgUnitName:str):\n",
    "    from datetime import datetime\n",
    "    \"\"\"\n",
    "    :param DataSetName: used to find the id of the dataset\n",
    "    :param OrgUnitName: used to find the id of the Organization units\n",
    "                        need the exact structure in dhis for example \"Sierra Leone\",\"any other orgunit inside\"\n",
    "    :param DataValueSets: data vec with this format { \"dataElement\": \"dataelement id\", \"value\": \"value to upload\" }\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    # Getting data set id for payload\n",
    "    DataSetLoaded = json.loads(getDataSetInfoByName(DataSetName))\n",
    "    DataSetId = DataSetLoaded['dataSets'][0]['id']\n",
    "    changeLog(f\"[FETCHING DATA-SET ID] >> \" + DataSetId)\n",
    "\n",
    "    # Getting orgUnit id for payload\n",
    "    OrgUnitLoaded = json.loads(getOrgUnitInfoByName(OrgUnitName))\n",
    "    OrgUnitId = OrgUnitLoaded['organisationUnits'][0]['id']\n",
    "    changeLog(f\"[FETCHING ORG-UNIT ID] >> \" + OrgUnitId)\n",
    "\n",
    "    # Process data value sets for get all ids instead his name\n",
    "    changeLog(f\"[PROCESSING DATA] >> \" + \"Converting dataelement name to data element id\")\n",
    "    #DataValueSets_loaded = json.loads(DataValueSets)\n",
    "    dt_info = json.loads(getDataElementInfoByName(DataElement))\n",
    "    dt_id = dt_info['dataElements'][0]['id']\n",
    "    # Convertir la cadena a un objeto datetime\n",
    "    date_obj = datetime.strptime(DataValueSets[\"date\"], \"%Y-%m-%d\")\n",
    "\n",
    "    # Convertir el objeto datetime al formato deseado\n",
    "    formatted_date = date_obj.strftime(\"%YW%U\")\n",
    "    payload = {\n",
    "        \"dataSet\": DataSetId,\n",
    "        \"completeDate\": DataValueSets[\"date\"],\n",
    "        \"period\": formatted_date,\n",
    "        \"orgUnit\": OrgUnitId,\n",
    "        \"dataValues\": [{\"dataElement\":dt_id,\"value\":DataValueSets[\"Value\"]}]\n",
    "    }\n",
    "    try:\n",
    "        session = login()\n",
    "        url = \"http://localhost:8080/api/dataValueSets\"\n",
    "        response = session.post(url, json=payload, headers={'content-type': 'application/json'})\n",
    "        changeLog(\"[INSERT DATA-VALUE-SET] >> \"+response.text)\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6510d28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.465510954Z",
     "start_time": "2023-07-19T11:53:07.450498471Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDataValueSetByDataSetID(datasetID:str):\n",
    "    try:\n",
    "        session = login()\n",
    "        url = f\"http://localhost:8080/api/dataSets/{datasetID}/dataValueSet\"\n",
    "        response = session.get(url)\n",
    "        changeLog(\"[FETCHING DATA-VALUE-SET INFO] >> \"+ response.text)\n",
    "        return response.text\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ffa6ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.465856403Z",
     "start_time": "2023-07-19T11:53:07.455311046Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCategoryOptionCombo(datasetID:str,dataElementID:str):\n",
    "    DataValueSetLoaded = json.loads(getDataValueSetByDataSetID(datasetID))\n",
    "    for i in DataValueSetLoaded['dataValues']:\n",
    "        if i[\"dataElement\"] == dataElementID:\n",
    "            return i[\"categoryOptionCombo\"]\n",
    "        else: \n",
    "            changeLog(f\"Cant find categoryOptionCombo for data element {dataElementID} in dataset {datasetID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a95936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.473125900Z",
     "start_time": "2023-07-19T11:53:07.470276417Z"
    }
   },
   "outputs": [],
   "source": [
    "def addDataValue_one_Value_test(value:int, date):\n",
    "    import json\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    OrgUnitName = \"Canarias\"\n",
    "    DataElement = \"IDS - invented (Deaths) \"\n",
    "    DataSetName = 'new_dataset'\n",
    "    DataElement = 'Yellow Fever (Suspected cases)'\n",
    "    DataSetName = 'IDS - Report: Suspected, Confirm, Death'\n",
    "    \"\"\"\n",
    "    :param DataSetName: used to find the id of the dataset\n",
    "    :param OrgUnitName: used to find the id of the Organization units\n",
    "                        need the exact structure in dhis for example \"Sierra Leone\",\"any other orgunit inside\"\n",
    "    :param DataValueSets: data vec with this format { \"dataElement\": \"dataelement id\", \"value\": \"value to upload\" }\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    # Getting data set id for payload\n",
    "    DataSetLoaded = json.loads(getDataSetInfoByName(DataSetName))\n",
    "    DataSetId = DataSetLoaded['dataSets'][0]['id']\n",
    "    changeLog(f\"[FETCHING DATA-SET ID] >> \" + DataSetId)\n",
    "\n",
    "    OrgUnitLoaded = json.loads(getOrgUnitInfoByName(OrgUnitName))\n",
    "    OrgUnitId = OrgUnitLoaded['organisationUnits'][0]['id']\n",
    "    changeLog(f\"[FETCHING ORG-UNIT ID] >> \" + OrgUnitId)\n",
    "\n",
    "    # Process data value sets for get all ids instead his name\n",
    "    changeLog(f\"[PROCESSING DATA] >> \" + \"Converting dataelement name to data element id\")\n",
    "    #DataValueSets_loaded = json.loads(DataValueSets)\n",
    "    dt_info = json.loads(getDataElementInfoByName(DataElement))\n",
    "    dt_id = dt_info['dataElements'][0]['id']\n",
    "    # Convertir la cadena a un objeto datetime\n",
    "    date_obj = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    print(DataSetId)\n",
    "    categoryOptionComboID = getCategoryOptionCombo(DataSetId, dt_id)\n",
    "  # Convertir el objeto datetime al formato deseado\n",
    "    formatted_date = date_obj.strftime(\"%YW%U\")\n",
    "    payload = {\n",
    "    \"dataSet\": DataSetId,\n",
    "    \"completeDate\": date,\n",
    "    \"orgUnit\": OrgUnitId,\n",
    "    \"dataValues\": \n",
    "        [{'dataElement': dt_id,\n",
    "       'categoryOptionCombo': categoryOptionComboID,\n",
    "       'period': formatted_date,\n",
    "       'value': value,\n",
    "        'followup': True}]\n",
    "    }\n",
    "    try:\n",
    "        session = login()\n",
    "        url = \"http://localhost:8080/api/dataValueSets\"\n",
    "        response = session.post(url, json=payload, headers={'content-type': 'application/json'})\n",
    "        changeLog(\"[INSERT DATA-VALUE-SET] >> \"+response.text)\n",
    "        return response\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "893348e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.481537853Z",
     "start_time": "2023-07-19T11:53:07.477396120Z"
    }
   },
   "outputs": [],
   "source": [
    "# response = addDataValue_one_Value_test(10,\"2023-07-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e346b432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.505463492Z",
     "start_time": "2023-07-19T11:53:07.488132065Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def generate_day_a_week(year,week_day_number):\n",
    "    start_date = datetime.date(year, 1, 1)  # primer día del año\n",
    "    end_date = datetime.date(year, 12, 31)  # último día del año\n",
    "    delta = datetime.timedelta(days=1)\n",
    "\n",
    "    week_day = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() == week_day_number:  # 0 es lunes\n",
    "            week_day.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date += delta\n",
    "\n",
    "    return week_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8240bb39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.539496647Z",
     "start_time": "2023-07-19T11:53:07.496137206Z"
    }
   },
   "outputs": [],
   "source": [
    "def export_random_data_test(orgUnit:str,years:list):\n",
    "    import pandas as pd\n",
    "    dataElement = 'Yellow Fever (Suspected cases)'\n",
    "    dataSet = 'IDS - Report: Suspected, Confirm, Death'\n",
    "    orgUnit = orgUnit  \n",
    "    file = \"extrapolated_data.csv\"\n",
    "    data = pd.read_csv(file)\n",
    "    # Convertir la columna 'Year' a formato de fecha y hora\n",
    "    data['Year'] = pd.to_datetime(data['Year'])\n",
    "    week_days = list()\n",
    "    for y in years:\n",
    "        week_days.extend(generate_day_a_week(y,1))\n",
    "    data = data[:len(week_days)]\n",
    "    data[\"date\"] = week_days\n",
    "    data.apply(addDataValue, axis = 1, args=(dataElement,dataSet,orgUnit,))\n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3457c1e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.539814033Z",
     "start_time": "2023-07-19T11:53:07.538472085Z"
    }
   },
   "outputs": [],
   "source": [
    "def impoer_payload(payload):\n",
    "    import pandas as pd\n",
    "    dataElement = 'Yellow Fever (Suspected cases)'\n",
    "    dataSet = 'IDS - Report: Suspected, Confirm, Death'\n",
    "    orgUnit = orgUnit  \n",
    "    file = \"extrapolated_data.csv\"\n",
    "    data = pd.read_csv(file)\n",
    "    # Convertir la columna 'Year' a formato de fecha y hora\n",
    "    data['Year'] = pd.to_datetime(data['Year'])\n",
    "    week_days = list()\n",
    "    for y in years:\n",
    "        week_days.extend(generate_day_a_week(y,1))\n",
    "    data = data[:len(week_days)]\n",
    "    data[\"date\"] = week_days\n",
    "    data.apply(addDataValue, axis = 1, args=(dataElement,dataSet,orgUnit,))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f136b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T11:53:07.539987308Z",
     "start_time": "2023-07-19T11:53:07.538769703Z"
    }
   },
   "outputs": [],
   "source": [
    "def  get_dataValues(dataSet, orgUnit):\n",
    "    datasetID = json.loads(getDataSetInfoByName(dataSet))[\"dataSets\"][0][\"id\"]\n",
    "    orgUnitID = json.loads(getOrgUnitInfoByName(orgUnit))[\"organisationUnits\"][0][\"id\"]\n",
    "\n",
    "    try:\n",
    "        session = login()\n",
    "        url = f\"http://localhost:8080/api/dataValueSets.json?dataSet={datasetID}&period=2023W1&orgUnit={orgUnitID}\"\n",
    "#         params = {\n",
    "#             \"filter\": f\"dataSet:like:{datasetID}&orgUnit:like:{orgUnitID}\"\n",
    "#         }\n",
    "        response = session.get(url) \n",
    "                               #headers={'content-type': 'application/json'}, params=params)\n",
    "        changeLog(f\"[FETCHING DATA-ELEMENT INFO] >> {response.text}\")\n",
    "        return response.text\n",
    "    except Exception:\n",
    "        changeLog(\"Unspected exception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1658d359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T12:18:35.112156484Z",
     "start_time": "2023-07-19T12:18:14.211627444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Year  Value        date\n0  2007-01-01    905  2023-01-03\n1  2007-01-01   2442  2023-01-10\n2  2007-01-01   2223  2023-01-17\n3  2007-01-01   2839  2023-01-24\n4  2007-01-01   2736  2023-01-31\n5  2007-01-01   2565  2023-02-07\n6  2007-01-01   2061  2023-02-14\n7  2007-01-01   2344  2023-02-21\n8  2007-01-01   1959  2023-02-28\n9  2007-01-01   1925  2023-03-07\n10 2007-01-01   1908  2023-03-14\n11 2007-01-01   3241  2023-03-21\n12 2007-01-01    699  2023-03-28\n13 2007-01-01   1806  2023-04-04\n14 2007-01-01   1947  2023-04-11\n15 2007-01-01   2424  2023-04-18\n16 2007-01-01   1799  2023-04-25\n17 2007-01-01   1809  2023-05-02\n18 2007-01-01   2609  2023-05-09\n19 2007-01-01   3216  2023-05-16\n20 2007-01-01   2005  2023-05-23\n21 2007-01-01   2157  2023-05-30\n22 2007-01-01   1400  2023-06-06\n23 2007-01-01   3401  2023-06-13\n24 2007-01-01   1065  2023-06-20\n25 2007-01-01   2607  2023-06-27\n26 2007-01-01   2648  2023-07-04\n27 2007-01-01   1345  2023-07-11\n28 2007-01-01   1405  2023-07-18\n29 2007-01-01   2173  2023-07-25\n30 2007-01-01   1796  2023-08-01\n31 2007-01-01   1620  2023-08-08\n32 2007-01-01   2462  2023-08-15\n33 2007-01-01   2268  2023-08-22\n34 2007-01-01   1628  2023-08-29\n35 2007-01-01   2401  2023-09-05\n36 2007-01-01   1581  2023-09-12\n37 2007-01-01   2441  2023-09-19\n38 2007-01-01   2294  2023-09-26\n39 2007-01-01   2480  2023-10-03\n40 2007-01-01   2023  2023-10-10\n41 2007-01-01   3193  2023-10-17\n42 2007-01-01   2610  2023-10-24\n43 2007-01-01   2008  2023-10-31\n44 2007-01-01   1721  2023-11-07\n45 2007-01-01   2545  2023-11-14\n46 2007-01-01   1177  2023-11-21\n47 2007-01-01   3068  2023-11-28\n48 2007-01-01   2617  2023-12-05\n49 2007-01-01   1102  2023-12-12\n50 2007-01-01   3059  2023-12-19\n51 2007-01-01   2565  2023-12-26",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Value</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2007-01-01</td>\n      <td>905</td>\n      <td>2023-01-03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2007-01-01</td>\n      <td>2442</td>\n      <td>2023-01-10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2007-01-01</td>\n      <td>2223</td>\n      <td>2023-01-17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2007-01-01</td>\n      <td>2839</td>\n      <td>2023-01-24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2007-01-01</td>\n      <td>2736</td>\n      <td>2023-01-31</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2007-01-01</td>\n      <td>2565</td>\n      <td>2023-02-07</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2007-01-01</td>\n      <td>2061</td>\n      <td>2023-02-14</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2007-01-01</td>\n      <td>2344</td>\n      <td>2023-02-21</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2007-01-01</td>\n      <td>1959</td>\n      <td>2023-02-28</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2007-01-01</td>\n      <td>1925</td>\n      <td>2023-03-07</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2007-01-01</td>\n      <td>1908</td>\n      <td>2023-03-14</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2007-01-01</td>\n      <td>3241</td>\n      <td>2023-03-21</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2007-01-01</td>\n      <td>699</td>\n      <td>2023-03-28</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2007-01-01</td>\n      <td>1806</td>\n      <td>2023-04-04</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2007-01-01</td>\n      <td>1947</td>\n      <td>2023-04-11</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2007-01-01</td>\n      <td>2424</td>\n      <td>2023-04-18</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2007-01-01</td>\n      <td>1799</td>\n      <td>2023-04-25</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2007-01-01</td>\n      <td>1809</td>\n      <td>2023-05-02</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2007-01-01</td>\n      <td>2609</td>\n      <td>2023-05-09</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2007-01-01</td>\n      <td>3216</td>\n      <td>2023-05-16</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2007-01-01</td>\n      <td>2005</td>\n      <td>2023-05-23</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2007-01-01</td>\n      <td>2157</td>\n      <td>2023-05-30</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2007-01-01</td>\n      <td>1400</td>\n      <td>2023-06-06</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2007-01-01</td>\n      <td>3401</td>\n      <td>2023-06-13</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2007-01-01</td>\n      <td>1065</td>\n      <td>2023-06-20</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2007-01-01</td>\n      <td>2607</td>\n      <td>2023-06-27</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2007-01-01</td>\n      <td>2648</td>\n      <td>2023-07-04</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2007-01-01</td>\n      <td>1345</td>\n      <td>2023-07-11</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2007-01-01</td>\n      <td>1405</td>\n      <td>2023-07-18</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2007-01-01</td>\n      <td>2173</td>\n      <td>2023-07-25</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>2007-01-01</td>\n      <td>1796</td>\n      <td>2023-08-01</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>2007-01-01</td>\n      <td>1620</td>\n      <td>2023-08-08</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>2007-01-01</td>\n      <td>2462</td>\n      <td>2023-08-15</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2007-01-01</td>\n      <td>2268</td>\n      <td>2023-08-22</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>2007-01-01</td>\n      <td>1628</td>\n      <td>2023-08-29</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>2007-01-01</td>\n      <td>2401</td>\n      <td>2023-09-05</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>2007-01-01</td>\n      <td>1581</td>\n      <td>2023-09-12</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>2007-01-01</td>\n      <td>2441</td>\n      <td>2023-09-19</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>2007-01-01</td>\n      <td>2294</td>\n      <td>2023-09-26</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>2007-01-01</td>\n      <td>2480</td>\n      <td>2023-10-03</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>2007-01-01</td>\n      <td>2023</td>\n      <td>2023-10-10</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>2007-01-01</td>\n      <td>3193</td>\n      <td>2023-10-17</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>2007-01-01</td>\n      <td>2610</td>\n      <td>2023-10-24</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>2007-01-01</td>\n      <td>2008</td>\n      <td>2023-10-31</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>2007-01-01</td>\n      <td>1721</td>\n      <td>2023-11-07</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>2007-01-01</td>\n      <td>2545</td>\n      <td>2023-11-14</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>2007-01-01</td>\n      <td>1177</td>\n      <td>2023-11-21</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>2007-01-01</td>\n      <td>3068</td>\n      <td>2023-11-28</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>2007-01-01</td>\n      <td>2617</td>\n      <td>2023-12-05</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>2007-01-01</td>\n      <td>1102</td>\n      <td>2023-12-12</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>2007-01-01</td>\n      <td>3059</td>\n      <td>2023-12-19</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>2007-01-01</td>\n      <td>2565</td>\n      <td>2023-12-26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_random_data_test(\"Trarza\", [2023])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2023-07-19 13:18:34.942827: [FETCHING DATA-ELEMENT INFO] >> {\"pager\":{\"page\":1,\"total\":1,\"pageSize\":50,\"pageCount\":1},\"dataElements\":[{\"displayName\":\"IDS - Yellow Fever (Suspected cases)\",\"id\":\"Q1xDDDusLwl\"}]}\n",
    "\n",
    "2023-07-19 13:18:35.067180: [INSERT DATA-VALUE-SET] >> {\"httpStatus\":\"Conflict\",\"httpStatusCode\":409,\"status\":\"WARNING\",\"message\":\"One more conflicts encountered, please check import summary.\",\"response\":{\"responseType\":\"ImportSummary\",\"status\":\"WARNING\",\"importOptions\":{\"idSchemes\":{},\"dryRun\":false,\"async\":false,\"importStrategy\":\"CREATE_AND_UPDATE\",\"mergeMode\":\"REPLACE\",\"reportMode\":\"FULL\",\"skipExistingCheck\":false,\"sharing\":false,\"skipNotifications\":false,\"skipAudit\":false,\"datasetAllowsPeriods\":false,\"strictPeriods\":false,\"strictDataElements\":false,\"strictCategoryOptionCombos\":false,\"strictAttributeOptionCombos\":false,\"strictOrganisationUnits\":false,\"strictDataSetApproval\":false,\"strictDataSetLocking\":false,\"strictDataSetInputPeriods\":false,\"requireCategoryOptionCombo\":false,\"requireAttributeOptionCombo\":false,\"skipPatternValidation\":false,\"ignoreEmptyCollection\":false,\"force\":false,\"firstRowIsHeader\":true,\"skipLastUpdated\":false,\"mergeDataValues\":false,\"skipCache\":false},\"description\":\"Import process completed successfully\",\"importCount\":{\"imported\":0,\"updated\":0,\"ignored\":1,\"deleted\":0},\"conflicts\":[{\"objects\":{\"dataElement\":\"Q1xDDDusLwl\",\"dataSet\":\"ZyZmZTUwctj\"},\"value\":\"Period: `null` is after latest open future period: `2023W29` for data element: `Q1xDDDusLwl` and data set: `ZyZmZTUwctj`\",\"errorCode\":\"E7641\",\"property\":\"period\",\"indexes\":[0]}],\"dataSetComplete\":\"2023-12-26\"}}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def generar_numero_ponderado(n, media):\n",
    "    numeros = list(range(1, n+1))\n",
    "    probabilidades = [1 / abs(x - media) if x != media else 1 for x in numeros]\n",
    "    total_probabilidades = sum(probabilidades)\n",
    "    ponderaciones = [p / total_probabilidades for p in probabilidades]\n",
    "\n",
    "    numero_aleatorio = random.choices(numeros, weights=ponderaciones)[0]\n",
    "    return numero_aleatorio\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:21:01.958449313Z",
     "start_time": "2023-07-20T15:21:01.914685884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "l = [generar_numero_ponderado(10,1) for n in range(10)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T15:29:53.258653924Z",
     "start_time": "2023-07-20T15:29:53.217493601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
